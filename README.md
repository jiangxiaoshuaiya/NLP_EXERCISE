# NLP_EXERCISE

+ bert demoï¼Œrequirements: torch, transformers

+ impelment of transformer: scaled dot-product attention ==> multi-head attention

+ impelment of position-wise feed-forward network: linear ==> relu ==> linear

  (maybe process with layer normalization and residual is better)