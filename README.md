# NLP_EXERCISE

+ bert demoï¼Œrequirements: torch, transformers
+ impelment of transformer: scaled dot-product attention ==> multi-head attention