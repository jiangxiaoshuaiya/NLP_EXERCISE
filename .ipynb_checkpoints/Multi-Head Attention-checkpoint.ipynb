{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "062334ee-04ba-47a4-8d9c-c045050e2d22",
    "_uuid": "144b9cd9-bf00-4c0e-be15-12befac1dbfb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,dim=512, heads=8, dropout=0):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.heads = heads\n",
    "        self.per_dim = dim / hesds\n",
    "        self.linear_q = nn.Linear(dim, self.per_dim * heads)\n",
    "        self.linear_k = nn.Linear(dim, self.per_dim * heads)\n",
    "        self.linear_v = nn.Linear(dim, self.per_dim * heads)\n",
    "        \n",
    "        self.ScaledDotProduct = ScaledDotProductAttention(scale=(dim/head)**0.5)\n",
    "        self.linear_final = nn.Linear(self.per_dim * heads, dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(dim)\n",
    "        \n",
    "        \n",
    "    def foward(self, Q, K, V):\n",
    "        \n",
    "        # 残差连接\n",
    "        residual = Q\n",
    "        heads = self.heads\n",
    "        per_dim = self.per_dim\n",
    "        batch_size = key.size(0)\n",
    "        \n",
    "        # linear projection\n",
    "        q = self.linear_q(Q)\n",
    "        k = self.linear_k(K)\n",
    "        v = self.linear_v(V)\n",
    "        \n",
    "        # split by heads\n",
    "        q = q.view(batch_size * heads, -1, per_dim)\n",
    "        k = k.view(batch_size * heads, -1, per_dim)\n",
    "        v = v.view(batch_size * heads, -1, per_dim)\n",
    "        \n",
    "        # scaled dot product attention\n",
    "        context, attention = self.ScaledDotProduct(q, k, v)\n",
    "        \n",
    "        # concat + linear + layernorm\n",
    "        context = context.view(batch_size, -1, dim_per_head * num_heads)\n",
    "        output = self.linear_final(context)\n",
    "        output = self.dropout(output)\n",
    "        output = self.layer_norm(output + residual)\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
