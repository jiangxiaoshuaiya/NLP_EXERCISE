# -*- coding: utf-8 -*-
"""adversarial_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wa0cyT8BDgWLEItUkIj3QZLsOUwS8nKP
"""

!pip install transformers
import os
import time
import torch 
import datetime
import numpy as np
import pandas as pd
import torch.nn as nn
from sklearn.metrics import f1_score, accuracy_score
from transformers import BertModel, BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup
from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler

PATH = 'drive/MyDrive/drive'
EPOCHS = 5
BATCH_SIZE = 32
MAX_LENGTH = 256
LEARNING_RATE = 1e-5

def load_data(train_test='train'):
    D = {}
    with open(os.path.join(PATH, train_test, train_test + '.query.tsv')) as f:
        for l in f:
            span = l.strip().split('\t')
            D[span[0]] = {'query': span[1], 'reply': []}

    with open(os.path.join(PATH, train_test, train_test + '.reply.tsv')) as f:
        for l in f:
            span = l.strip().split('\t')
            if len(span) == 4:
                q_id, r_id, r, label = span
            else:
                label = None
                q_id, r_id, r = span
            D[q_id]['reply'].append([r_id, r, label])
    d = []
    for k, v in D.items():
        q_id = k
        q = v['query']
        reply = v['reply']

        for r in reply:
            r_id, rc, label = r

            d.append([q_id, q, r_id, rc, label])
    return d

train = load_data(train_test='train')
test = load_data(train_test='test')
print('train example', train[0:5])
print('test example', test[0:5])

def process(context, threshold=None):
  '''
  Parameters@
    data: list, a matrix which dimensional equals to two
    threshold: float, use to split train data, range from 0 to 1
  
  Returns@
    dataloader of train set and valid set while need to split data, otherwise, test set
  '''
  input_ids = []
  attention_masks = []
  token_type_ids = []
  labels = []
  
  for text in context:
    encode_dic = tokenizer.encode_plus(
      text=text[1],
      text_pair=text[3],
      max_length=MAX_LENGTH,
      padding='max_length',
      truncation=True,
      pad_to_multiple_of=True,  
      add_special_tokens=True,
      return_attention_mask=True,
      return_token_type_ids=True,  
      return_tensors='pt'
    )
    input_ids.append(encode_dic['input_ids'])
    attention_masks.append(encode_dic['attention_mask'])
    token_type_ids.append(encode_dic['token_type_ids'])
    labels.append([int(text[4])] if text[4] is not None else [-1])

  input_ids = torch.cat(input_ids, dim=0)
  attention_masks = torch.cat(attention_masks, dim=0)
  token_type_ids = torch.cat(token_type_ids, dim=0)
  labels = torch.tensor(labels)

  # pack up and transform to form of dataloader, which is feeded to model, 
  # return train set and vaild set while  threshold is not None,
  # otherwise, predict dataloader
  dataset = TensorDataset(input_ids, token_type_ids, attention_masks, labels)

  if threshold:

    if threshold>0 or threshold<1:
      train_size = int(threshold * len(dataset))
      val_size = len(dataset) - train_size
      train_data, val_data = random_split(dataset, [train_size, val_size])

      train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, sampler=RandomSampler(train_data))
      val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, sampler=SequentialSampler(val_data))

      return train_loader, val_loader

    else:
      print('NumericalError: threshold out of range, a correct value must between (0,1)')
  else:

    predict_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=SequentialSampler(dataset))

    return predict_loader

tokenizer = BertTokenizer.from_pretrained('hfl/chinese-bert-wwm')
train_dataloader, valid_dataloader = process(train, threshold=0.8)
test_dataloader = process(test)
print('data processing is done !!')

# Adversarial_Training
class FGM():
    def __init__(self, model):
        self.model = model
        self.backup = {}

    def attack(self, epsilon=1., emb_name='embeddings.word_embeddings.weight'):
        for name, param in self.model.named_parameters():
            # named word_embeddings and do have gradient
            if param.requires_grad and emb_name in name:
                self.backup[name] = param.data.clone()
                norm = torch.norm(param.grad)
                if norm != 0:
                    r_at = epsilon * param.grad / norm
                    param.data.add_(r_at)

    def restore(self, emb_name='embeddings.word_embeddings.weight'):
        for name, param in self.model.named_parameters():
            if param.requires_grad and emb_name in name: 
                assert name in self.backup
                param.data = self.backup[name]
        self.backup = {}

def format_time(elapsed):    
  elapsed_rounded = int(round((elapsed)))
  return str(datetime.timedelta(seconds=elapsed_rounded))

def flat_accuracy(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return accuracy_score(labels_flat, pred_flat)

def flat_f1(preds, labels):
    pred_flat = np.argmax(preds, axis=1).flatten()
    labels_flat = labels.flatten()
    return f1_score(labels_flat, pred_flat)

def train(model, dataloader):
  
  model.cuda()
  model.train()
  for epoch in range(EPOCHS):

    print('======== Epoch {:} / {:} ========'.format(epoch + 1, EPOCHS))
    print('Training...')
    start_train = time.time()

    loss = 0
    
    total_train_loss = 0
    for step, batch in enumerate(dataloader):
      torch.cuda.empty_cache()

      if step % 50 ==0:
        elapsed = format_time(time.time() - start_train)
        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))

      batch_input_ids = batch[0].cuda()
      batch_token_type_ids = batch[1].cuda()
      batch_attention_masks = batch[2].cuda()
      batch_labels = batch[3].cuda()

      output = model(batch_input_ids, batch_attention_masks, batch_token_type_ids, labels=batch_labels)
      
      model.zero_grad()
      output.loss.backward()
      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
      
      optimizer.step()
      scheduler.step()
      
      total_train_loss += loss
        
    average_train_loss = total_train_loss / len(train_dataloader)
    training_time = format_time(time.time() - start_train)

    print("  Average training loss: {0:.2f}".format(average_train_loss))
    print("  Training epcoh took: {:}".format(training_time))

      
# def eval(model, dataloader):
#   pass

# def predict(model, dataloader):
#   pass

model = BertForSequenceClassification.from_pretrained('hfl/chinese-bert-wwm')
optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)
total_steps = len(train_dataloader) * EPOCHS
# change learning rate dynamically in total steps, 
# during warmup phase and train period
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

torch.cuda.empty_cache()
train(model, train_dataloader)
!nvidia-smi

logits

output[0]

output[1]









